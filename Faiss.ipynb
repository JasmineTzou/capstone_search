{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ljLauk1KaTo7","executionInfo":{"status":"ok","timestamp":1698359103921,"user_tz":300,"elapsed":8762,"user":{"displayName":"Zoey Chen","userId":"08987474327962258210"}},"outputId":"e8cffd4b-6a6e-4203-8e94-11c2e088ca21"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting faiss-cpu\n","  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: faiss-cpu\n","Successfully installed faiss-cpu-1.7.4\n"]}],"source":["!pip install faiss-cpu --no-cache"]},{"cell_type":"code","source":["\n","\n","! pip install sentence_transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u6d1ll0Wbl_U","executionInfo":{"status":"ok","timestamp":1698359119728,"user_tz":300,"elapsed":15810,"user":{"displayName":"Zoey Chen","userId":"08987474327962258210"}},"outputId":"10a6613a-509c-4a92-ecd6-f4e15a9f3828"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sentence_transformers\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting transformers<5.0.0,>=4.6.0 (from sentence_transformers)\n","  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.0+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.3)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n","Collecting sentencepiece (from sentence_transformers)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence_transformers)\n","  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n","Collecting tokenizers<0.15,>=0.14 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n","Collecting huggingface-hub>=0.4.0 (from sentence_transformers)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n","Building wheels for collected packages: sentence_transformers\n","  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=e98bf0488c3df4ecbeba7653542c49c43f7ac42298b73de14fe6386529495fd8\n","  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n","Successfully built sentence_transformers\n","Installing collected packages: sentencepiece, safetensors, huggingface-hub, tokenizers, transformers, sentence_transformers\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 sentence_transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.14.1 transformers-4.34.1\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"W3u639pnaTpA","executionInfo":{"status":"ok","timestamp":1698359120037,"user_tz":300,"elapsed":312,"user":{"displayName":"Zoey Chen","userId":"08987474327962258210"}}},"outputs":[],"source":["import numpy as np\n","import faiss\n","import requests\n","from io import StringIO\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"H5MnVUE3aTpB"},"source":["# Dataset Initialization"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zPkrdbBcaf9b","executionInfo":{"status":"ok","timestamp":1698359142099,"user_tz":300,"elapsed":22066,"user":{"displayName":"Zoey Chen","userId":"08987474327962258210"}},"outputId":"9d53a383-78b2-4f5e-a9f8-3072f5afbf7f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["df = pd.read_json(\"/content/drive/Capstone-esya.ai/DATA/summarized_df100.json\")\n","df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":346},"id":"VtbRgVKnam9X","executionInfo":{"status":"error","timestamp":1698359193301,"user_tz":300,"elapsed":225,"user":{"displayName":"Zoey Chen","userId":"08987474327962258210"}},"outputId":"f43b22fb-fd92-48e0-ae0a-b5fb3f569552"},"execution_count":5,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-8acefa59b185>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/Capstone-esya.ai/DATA/summarized_df100.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0mconvert_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m     json_reader = JsonReader(\n\u001b[0m\u001b[1;32m    734\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors)\u001b[0m\n\u001b[1;32m    816\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows can only be passed if lines=True\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data_from_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_data_from_filepath\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         ):\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"File {filepath_or_buffer} does not exist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: File /content/drive/Capstone-esya.ai/DATA/summarized_df100.json does not exist"]}]},{"cell_type":"markdown","metadata":{"id":"_4ZKFnb3aTpF"},"source":["We will take all samples from `text` and build sentence embeddings for each - which we can then store in FAISS."]},{"cell_type":"code","source":["#select only text for futher embedding\n","sentences = df['text'].tolist()\n","sentences[:5]"],"metadata":{"id":"ltAzJb9abGrW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(set(sentences))"],"metadata":{"id":"zutlnCh0bRnM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tokenized the words"],"metadata":{"id":"qB6JUQ4k_nrj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ThMbePFbaTpI"},"outputs":[],"source":["from sentence_transformers import SentenceTransformer\n","\n","model = SentenceTransformer('bert-base-nli-mean-tokens')\n","\n","sentence_embeddings = model.encode(sentences)\n","sentence_embeddings.shape"]},{"cell_type":"markdown","metadata":{"id":"wDog7hz0aTpI"},"source":["We can save/load from file in the case of needing to reload the notebook for any reason later."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i3EGP0sIaTpI"},"outputs":[],"source":["sentence_embeddings.shape[0]"]},{"cell_type":"markdown","metadata":{"id":"3Nyv2k-CaTpJ"},"source":["We setup our FAISS database dimensionality (number of dimensions per vector) based on these vectors."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WL4nA8fKaTpO"},"outputs":[],"source":["d = sentence_embeddings.shape[1]\n","d"]},{"cell_type":"markdown","source":["# Flat L2 Index\n","\n","We initialize the flat L2 distance index `IndexFlatL2`, all we need is the specify the vector dimensionality - which in this case is `d == 768` (to align with the sentence-BERT model output embeddings of size `768`)."],"metadata":{"id":"edXUJOhvDAJ0"}},{"cell_type":"code","source":["index = faiss.IndexFlatL2(d)"],"metadata":{"id":"o_ptd8iTC4GA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4qWPy3-NaTpP"},"outputs":[],"source":["index.is_trained"]},{"cell_type":"markdown","metadata":{"id":"0Im2KaR4aTpQ"},"source":["Okay so once we're happy that our index is prepared, we then add new vectors using the `add` method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rcvt-UDbaTpQ"},"outputs":[],"source":["index.add(sentence_embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cmKZmOPTaTpQ"},"outputs":[],"source":["index.ntotal"]},{"cell_type":"markdown","metadata":{"id":"EqAheIsoaTpR"},"source":["Then search given a query `xq` and number of nearest neigbors to return `k`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QCZH1PQ8aTpR"},"outputs":[],"source":["k = 5\n","xq = model.encode([\"politics situation\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_gXQSEc_aTpR"},"outputs":[],"source":["%%time\n","D, I = index.search(xq, k)  # search\n","print(I)  # k-nearest neigbors of the query vector | nprobe == 1: 6495 26392 61709 49932 | nprobe == 10: 36245  6495 57489  8705"]},{"cell_type":"markdown","metadata":{"id":"wdC1FgrvaTpR"},"source":["Here we're returning text of the news article, which returns:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IosyEGZeaTpR"},"outputs":[],"source":["[f'{i}: {sentences[i]}' for i in I[0]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MBi9btxkaTpS"},"outputs":[],"source":["# Interactive search loop\n","while True:\n","    # Request user input for a query\n","    query_text = input(\"What do you want to search (or 'q' to quit): \")\n","\n","    # Check if the user wants to quit\n","    if query_text.lower() == 'q':\n","        print(\"Exiting the search - thank you\")\n","        break\n","\n","    # Perform similarity search\n","    k = 5  # Number of nearest neighbors to retrieve\n","    distances, indices = index.search(xq, k)\n","\n","    # Get similar texts\n","    similar_texts = df.iloc[indices[0]]\n","\n","    # Display the similar texts\n","    print(\"\\nSimilar Texts:\")\n","    print(similar_texts)"]},{"cell_type":"code","source":[],"metadata":{"id":"lcycWpTDE6kq"},"execution_count":null,"outputs":[]}],"metadata":{"interpreter":{"hash":"a683edd788238e5c64f9fa2e4bdd4387776bc5c6f4f0a84da0685f9a25e421d6"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}